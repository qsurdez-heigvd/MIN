{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üî®&nbsp;&nbsp;Google Colab Setup</font>\n",
        "Upload the data (without unzipping it) to your Google Drive to be able to access datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZC59s1KkK-zF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üéØ&nbsp;&nbsp;Your task</font>\n",
        "\n",
        "Take some time to play with this notebook (you will need to run the code this time) to gain a deeper understanding of the latent space. While there are no specific questions associated with this notebook, its content is important and it shouldn't take you too much time."
      ],
      "metadata": {
        "id": "_M1ZdftZn8Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UL_bX7KsK49c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/dcgan.zip"
      ],
      "metadata": {
        "id": "xd9NKNTZK7Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf_keras"
      ],
      "metadata": {
        "id": "7ElM-0MiXVur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üß∫&nbsp;&nbsp;Load Generator Model</font>"
      ],
      "metadata": {
        "id": "aSFrUZaFKBcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tf_keras as keras # We need to use tf_keras because the models saved were trained before Keras 3\n",
        "generator = keras.models.load_model(f'dcgan/generator_1', compile=False)"
      ],
      "metadata": {
        "id": "QLSXG8s6KAsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üé©&nbsp;&nbsp;Generate Some Images</font>"
      ],
      "metadata": {
        "id": "XM8OhPFGLTkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1emthN3wKhn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_images = 100\n",
        "\n",
        "# Remove seed if you want different images each time you run this cell\n",
        "random_latent_vectors = tf.random.normal(shape=(n_images, 128), seed=1000)\n",
        "generated_images = generator(random_latent_vectors)\n",
        "generated_images = np.uint8(generated_images.numpy() * 127.5 + 127.5)\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "for i in range(n_images):\n",
        "    plt.subplot(10, 10, i+1)\n",
        "    plt.subplots_adjust(top=0.8, hspace=0.6)\n",
        "    plt.title(str(i))\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(generated_images[i].squeeze(), cmap=\"gray\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üé®&nbsp;&nbsp;What Is Latent Space?</font>\n",
        "\n",
        "The latent space is a multi-dimensional space the encode data into more abstract features.\n",
        "\n",
        "Imagine you where to compare clothes images yourself, you would think about them with some abstractions (does it have sleeves? For which body part is it? ...) on some scales (sleeves lenght, ...) instead of comparing images pixels values one by one.\n",
        "\n",
        "That's what we're doing with machine learning. We learn a meaningful way to encode the data instead of working directly on the whole data and then we can either do classification, regression, or reconstruct back data.\n",
        "\n",
        "Most of the time the features it will learn will not be directly interpretable but some models architectures are designed in a way to have features we can understand (e.g. InfoGAN).\n",
        "\n",
        "![Latent Space](https://miro.medium.com/max/720/0*kHJ_LsPi-jz_CreZ.webp)\n",
        "\n",
        "In our case with DCGAN, we don't have the encoding part."
      ],
      "metadata": {
        "id": "De1afo54NeGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üî¨&nbsp;&nbsp;Exploring GAN Latent Space</font>"
      ],
      "metadata": {
        "id": "7YCBIuobI3Yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember that with DCGAN, input vectors values are latent variables: features describing the object that are \"hidden\" because we are not directly able to measure or observe them.\n",
        "\n",
        "We have as many dimensions in the latent space as we have values in the input vector. Thus a latent vector with two values form a space with two dimensions and one with 128 values form a space with 128 dimensions.\n",
        "\n",
        "Each images we generate are sampled from this latent space, all of them have a feature vector representation that lies in this feature space.\n",
        "\n",
        "When we understand that, we can as well understand that we can go from one generated image to another by moving from the first latent vector to the other one (we are moving from one point in the latent space to another one)."
      ],
      "metadata": {
        "id": "ZTi4InokI6DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_generated_from_latent_vector(latent_vector):\n",
        "  generated_image = generator(np.expand_dims(latent_vector, axis=0))[0]\n",
        "\n",
        "  print(\"Latent vector: \", latent_vector)\n",
        "\n",
        "  plt.title(\"Image generated\")\n",
        "  plt.imshow(np.uint8(generated_image.numpy() * 127.5 + 127.5).squeeze(), cmap=\"gray\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DqXifpHGI3-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature vector example\n",
        "\n",
        "You can see below an example of a feature vector and the image we've generated from it."
      ],
      "metadata": {
        "id": "jiETyzPu6Jyj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_generated_from_latent_vector(random_latent_vectors[0])"
      ],
      "metadata": {
        "id": "jbWHh85zJBV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"#59B4C3\">üåç&nbsp;&nbsp;Moving in the latent space from images to images</font>"
      ],
      "metadata": {
        "id": "scZfIfOXJHzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_linear_interpolation(vector_index1, vector_index2, vector_index3, n_step=10):\n",
        "  move_vector_x = (random_latent_vectors[vector_index2] - random_latent_vectors[vector_index1]) / n_step\n",
        "  move_vector_y = (random_latent_vectors[vector_index3] - random_latent_vectors[vector_index1]) / n_step\n",
        "\n",
        "  fig, axs = plt.subplots(n_step, n_step, figsize=(n_step, n_step))\n",
        "\n",
        "  axs[0, 0].set_title(vector_index1)\n",
        "  axs[0, n_step - 1].set_title(vector_index2)\n",
        "  axs[n_step - 1, 0].set_title(vector_index3, x=-0.25, y=0.3)\n",
        "\n",
        "  for i in np.arange(n_step):\n",
        "    for j in np.arange(n_step):\n",
        "      generated_image = generator(np.expand_dims(random_latent_vectors[vector_index1] + move_vector_y * i + move_vector_x * j, axis=0))[0]\n",
        "      axs[i,j].axis('off')\n",
        "      axs[i,j].imshow(np.uint8(generated_image.numpy() * 127.5 + 127.5).squeeze(), cmap=\"gray\")\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "H1eVZMNvJKI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose the latent vector indexes and see how we can travel from one image to another. The indexes are the same as the number shown in the part where we show some images generated with the generator model.\n",
        "\n",
        "The grid it will create below is like so: on the x axis we go from the first selected image (top left) to the second selected image (top right) by taking the difference between the two vectors and dividing this difference into _n_ number of steps. We're doing the same with the y axis, going from the first selected image (top left) to the third one (bottom left).\n",
        "\n",
        "On the diagonal, we're moving in both the direction of the second image and the direction of the third one.\n",
        "\n",
        "Click \"Run Interact\" to see the result."
      ],
      "metadata": {
        "id": "XPAqMkcc3Dcb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import interact_manual\n",
        "import ipywidgets as widgets\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "interact_manual(\n",
        "    plot_linear_interpolation,\n",
        "    vector_index1=widgets.BoundedIntText(min=0, max=99, step=1, value=0, style=style),\n",
        "    vector_index2=widgets.BoundedIntText(min=0, max=99, step=1, value=1, style=style),\n",
        "    vector_index3=widgets.BoundedIntText(min=0, max=99, step=1, value=2, style=style),\n",
        "    n_step=widgets.IntSlider(min=2, max=20, step=1, value=10)\n",
        ")"
      ],
      "metadata": {
        "id": "YRDUzP_VJMys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "See below an example of moving in the latent space of a stable diffusion model (similar to DALL-E):\n",
        "\n",
        ">Latent space walking, or latent space exploration, is the process of sampling a point in latent space and incrementally changing the latent representation. Its most common application is generating animations where each sampled point is fed to the decoder and is stored as a frame in the final animation. For high-quality latent representations, this produces coherent-looking animations. These animations can provide insight into the feature map of the latent space, and can ultimately lead to improvements in the training process. One such GIF is displayed below:\\\n",
        "![Latent Space Walk](https://keras.io/img/examples/generative/random_walks_with_stable_diffusion/panda2plane.gif)\\\n",
        ">Source: https://keras.io/examples/generative/random_walks_with_stable_diffusion/"
      ],
      "metadata": {
        "id": "OERfiCJ9rZ44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"#59B4C3\">üîß&nbsp;&nbsp;Selecting Features Values Manually</font>\n",
        "\n",
        "Here, you can see how it's not easy to understand the role of each features. Try to change the latent features values and to see the impact."
      ],
      "metadata": {
        "id": "TeLHMA9Hj6fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import Layout, HBox, VBox, interactive\n",
        "\n",
        "def select_feature_vector(**kwargs):\n",
        "  image_generated_from_latent_vector(list(kwargs.values()))\n",
        "\n",
        "features_widget_list = {\n",
        "    f\"feature_{idx}\": widgets.FloatSlider(min=-4, max=4, step=0.05, value=0)\n",
        "    for idx in np.arange(128)\n",
        "}\n",
        "\n",
        "box_layout = Layout(display='flex',\n",
        "                    flex_flow='row',\n",
        "                    align_items='stretch',\n",
        "                    width='70%')\n",
        "\n",
        "widget = interactive(\n",
        "    select_feature_vector,\n",
        "    {'manual': True},\n",
        "    **features_widget_list,\n",
        ")\n",
        "\n",
        "controls = HBox(widget.children[:-1], layout = Layout(flex_flow='row wrap'))\n",
        "output = widget.children[-1]\n",
        "output.layout.height = '350px'\n",
        "\n",
        "display(VBox([controls, output]))"
      ],
      "metadata": {
        "id": "MwvXIaGQAYmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TcVQbfANnpCo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}